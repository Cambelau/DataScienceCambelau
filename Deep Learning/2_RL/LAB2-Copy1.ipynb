{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1de4652",
   "metadata": {},
   "source": [
    "# Reinforcement Learning :\n",
    "\n",
    "## Pre-Lab : Value Iteration (VI)\n",
    "\n",
    "- Implement the V.I algorithm with the Bellman Equation \n",
    "$$ V^{\\pi}(s) = R(s) + \\gamma \\sum_{s'} P_{s\\pi(s)}(s')V^{\\pi}(s') $$\n",
    "\n",
    "- V.I algorithm, for each state compute : \n",
    "$$ V(S) = R(S) + \\gamma\\max_a \\sum_{S'} Psa(S')V(s') $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce89f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library pythons :\n",
    "import numpy as np\n",
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98dc5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the map of 11 cells\n",
    "S = [(0,0), (0,1), (0,2), (0,3),\n",
    "     (1,0),        (1,2), (1,3),\n",
    "     (2,0), (2,1), (2,2), (2,3) ]\n",
    "\n",
    "# All possible transitions\n",
    "S__ = {(0,0): [(0,0), (0,1), (1,0)],\n",
    "       (1,0): [(1,0), (0,0), (2,0)],\n",
    "       (2,0): [(2,0), (1,0), (2,1)],\n",
    "       (0,1): [(0,0), (0,1), (0,2)],\n",
    "       (0,2): [(0,1), (0,2), (0,3), (1,2)],\n",
    "       (2,3): [(2,2), (2,3), (1,3)],\n",
    "       (2,2): [(2,1), (2,2), (2,3), (1,2)],\n",
    "       (2,1): [(2,0), (2,1), (2,2)],\n",
    "       (1,2): [(0,2), (1,2), (2,2), (1,3)],\n",
    "       (1,3): [(1,3)],\n",
    "       (0,3): [(0,3)]}\n",
    "\n",
    "\n",
    "# Init the V matrix\n",
    "V = np.zeros((3,4))\n",
    "\n",
    "# Init the Action set\n",
    "A = ['N', 'S', 'E', 'W']\n",
    "\n",
    "# State transition distribution\n",
    "def Psa(s, a, s_prime):\n",
    "    prob_N = np.array([[0.0, 0.8, 0.0],\n",
    "                       [0.1, 0.0, 0.1],\n",
    "                       [0.0, 0.0, 0.0]])\n",
    "    \n",
    "    prob_S = np.array([[0.0, 0.0, 0.0],\n",
    "                       [0.1, 0.0, 0.1],\n",
    "                       [0.0, 0.8, 0.0]])\n",
    "    \n",
    "    prob_E = np.array([[0.0, 0.1, 0.0],\n",
    "                       [0.0, 0.0, 0.8],\n",
    "                       [0.0, 0.1, 0.0]])\n",
    "    \n",
    "    prob_W = np.array([[0.0, 0.1, 0.0],\n",
    "                       [0.8, 0.0, 0.0],\n",
    "                       [0.0, 0.1, 0.0]])\n",
    "    i = s_prime[0] - s[0] + 1\n",
    "    j = s_prime[1] - s[1] + 1\n",
    "    \n",
    "    # \n",
    "    if (a == 'N'):\n",
    "        return prob_N[i][j]\n",
    "    elif (a == 'S'):\n",
    "        return prob_S[i][j]\n",
    "    elif (a == 'E'):\n",
    "        return prob_E[i][j]\n",
    "    elif (a == 'W'):\n",
    "        return prob_W[i][j]\n",
    "    \n",
    "    \n",
    "# Reward function\n",
    "def R(s):\n",
    "    if (s == (0,3)):\n",
    "        return 1\n",
    "    elif (s == (1,3)):\n",
    "        return -1\n",
    "    else:\n",
    "        return -0.02\n",
    "    \n",
    "# The discount factor\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80758670",
   "metadata": {},
   "source": [
    "### Test of the Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "191ea0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "iterations = 1000\n",
    "\n",
    "while i < iterations:\n",
    "    for state in S:\n",
    "        max_actions = list()\n",
    "        for action in A:\n",
    "            s = 0\n",
    "            for s_next in S__.get(state):\n",
    "                s += Psa(state, action, s_next) * V[s_next[0], s_next[1]]\n",
    "            max_actions.append(s)\n",
    "        V[state[0], state[1]] = R(state) + gamma * max(max_actions)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86b7093",
   "metadata": {},
   "source": [
    "### The Optimal Value :\n",
    "$$ V \\to V^* $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc2ee47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5204131 ,  0.63331887,  0.82489757,  1.        ],\n",
       "       [ 0.39216717,  0.        ,  0.53431887, -1.        ],\n",
       "       [ 0.32482867,  0.34578052,  0.46184409,  0.24678052]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da8e672",
   "metadata": {},
   "source": [
    "### The Optimal Policy :\n",
    "$$ \\Pi^* = argmax_a \\sum_{S'} Psa(S')V^*(S')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4feceffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Policy\n",
    "PI = [['p', 'p', 'p', 'p'],\n",
    "      ['p', 'p', 'p', 'p'],\n",
    "      ['p', 'p', 'p', 'p']]\n",
    "\n",
    "for state in S:\n",
    "    max_actions2 = list()\n",
    "    for action in A:\n",
    "        s = 0\n",
    "        for s_next in S__.get(state):\n",
    "            s += Psa(state, action, s_next) * V[s_next[0], s_next[1]]\n",
    "        max_actions2.append(s)\n",
    "    PI[state[0]][state[1]] = A[np.argmax(max_actions2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c472d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['E', 'E', 'E', 'N'], ['N', 'p', 'N', 'N'], ['N', 'E', 'N', 'W']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
